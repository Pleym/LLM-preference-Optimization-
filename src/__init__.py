# LLM Preference Optimization: DPO vs SFT Comparison
"""
This project implements a rigorous comparison between Direct Preference Optimization (DPO)
and Supervised Fine-Tuning (SFT) for aligning language models with human preferences.

Key components:
- data/: Data loading and formatting for Anthropic HH-RLHF dataset
- Training scripts using TRL (Transformer Reinforcement Learning) library
- Evaluation metrics for measuring alignment quality
"""

__version__ = "0.1.0"
